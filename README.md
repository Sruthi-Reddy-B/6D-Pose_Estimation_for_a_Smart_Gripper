# 6D Pose Estimation for Smart Gripper

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Sruthi-Reddy-B/6d-pose-estimation-smart-gripper/blob/main/notebooks/pose_estimation_pipeline.ipynb)

## ğŸ¯ Overview
This project implements a **vision-based 6D pose estimation pipeline** for a robotic gripper, inspired by my Master's thesis  
**â€œVision-Driven Automation: Smart Gripper Development for Autonomous Material Manipulation.â€**

It detects an object using **YOLOv8** and estimates its **6D pose (position + orientation)** to assist robotic manipulation.

---

## ğŸ§  Tech Stack
- **Frameworks:** PyTorch, Ultralytics YOLOv8  
- **Libraries:** OpenCV, NumPy, Matplotlib, SciPy, Pandas  
- **Environment:** Google Colab (GPU recommended)

---

## ğŸ“‚ Folder Structure
```text
6d-pose-estimation-smart-gripper/
â”œâ”€â”€ data/           # RGB images and 6D pose labels
â”œâ”€â”€ notebooks/      # Pipeline notebook
â”œâ”€â”€ src/            # Training/inference scripts
â”œâ”€â”€ results/        # Detections and pose visualizations
â””â”€â”€ requirements.txt
```

âš™ï¸ How to Run
ğŸŸ¢ Google Colab (Recommended)

Click the Colab badge above.

Run all cells.

The notebook performs detection â†’ pose estimation â†’ visualization.

Results are saved in /results/.

ğŸ’» Local Setup
pip install -r requirements.txt
jupyter notebook notebooks/pose_estimation_pipeline.ipynb

ğŸ“Š Results

Object detected successfully with YOLOv8.

6D pose matrix estimated (x, y, z + rotation R).

Visualization overlays local coordinate axes on detected object.

These outputs simulate the workflow of an industrial vision-gripper system.

ğŸš€ Future Enhancements

Replace synthetic data with real RGB-D images.

Integrate with ROS 2 for real-time robot grasping.

Use YOLOv5-6D or PoseCNN for accurate orientation recovery.

Add point-cloud-based refinement (Gaussian splatting / NeRF).
